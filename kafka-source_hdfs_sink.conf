# kafka as source and hdfs as sink

# Name the components on this agent
a1.sources = r1
a1.channels = c1
a1.sinks = k1

# Describe the source
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.kafka.bootstrap.servers = localhost
a1.sources.r1.kafka.consumer.group.id = joey-flume
a1.sources.r1.kafka.topics = app-logs
a1.sources.r1.kafka.topics.regex = app-logs-topic
a1.sources.r1.batchSize = 50
# Set to true to read events as the Flume Avro binary format. Used in conjunction with the same property on the KafkaSink or with the parseAsFlumeEvent property on the Kafka Channel this will preserve any Flume headers sent on the producing side.
a1.sources.r1.useFlumeEventFormat = true

# Describe the sink
a1.sinks.k1.type = hdfs
# possibly s3 too ? 
a1.sinks.k1.hdfs.path = hdfs://namenode/flume/webdata/

# Describe the channel
# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
