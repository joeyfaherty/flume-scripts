# single-node Flume configuration, using kafka as source and s3 as sink

# Name the components on this agent
a1.sources = kafka-source
a1.channels = memory-channel
a1.sinks = s3-hdfs-sink log-sink

# Describe the source
a1.sources.kafka-source.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.kafka-source.kafka.bootstrap.servers = localhost:9092
a1.sources.kafka-source.kafka.topics = POC-test
a1.sources.kafka-source.kafka.topics.regex = POC-test
#a1.sources.kafka-source.zookeeperConnect = localhost:2191
#a1.sources.kafka-source.groupId = flume
#a1.sources.kafka-source.channels = memory-channel
#a1.sources.kafka-source.interceptors = i1
#a1.sources.kafka-source.interceptors.i1.type = timestamp
#a1.sources.kafka-source.kafka.consumer.timeout.ms = 100

# Define a sink that outputs to logger
a1.sinks.log-sink.channel = memory-channel
a1.sinks.log-sink.type = logger

# Define a sink that outputs to hdfs
a1.sinks.s3-hdfs-sink.channel = memory-channel
a1.sinks.s3-hdfs-sink.type = hdfs
a1.sinks.s3-hdfs-sink.hdfs.path = s3n://ACCESS_KEY:SECRET_KEY@logs-flume
a1.sinks.s3-hdfs-sink.hdfs.fileType=DataStream

# Describe the channel
# Use a channel which buffers events in memory
a1.channels.memory-channel.type = memory
a1.channels.memory-channel.capacity = 1000
a1.channels.memory-channel.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.kafka-source.channels = memory-channel
